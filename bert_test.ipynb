{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rhome/eingerman/mambaforge/envs/vits2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DebertaV2Model, DebertaV2Tokenizer\n",
    "MODEL_NAME = 'microsoft/deberta-v3-small'\n",
    "model = DebertaV2Model.from_pretrained(MODEL_NAME)\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[\"Hellow World\", \"Good nights\", \"What is going on?\"]\n",
    "encoded_input = tokenizer(texts, return_tensors='pt', padding=True)\n",
    "\n",
    "output = model(**encoded_input, output_hidden_states=True)\n",
    "bart_out = torch.cat(output[\"hidden_states\"][-3:-2], -1)[0]\n",
    "converted=tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhidden_states\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "output[\"hidden_states\"][-3:-2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"hidden_states\"][-3:-2][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m converted\u001b[38;5;241m=\u001b[39m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_input\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/vits2/lib/python3.10/site-packages/transformers/tokenization_utils.py:1047\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m   1045\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[0;32m-> 1047\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1048\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_ids:\n\u001b[1;32m   1049\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "converted=tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 51]) torch.Size([1, 51, 768]) 49\n"
     ]
    }
   ],
   "source": [
    "text = \"when these rays reach the observer direct, he sees the lamps or luminiferous bodies themselves, but when he is out of their direct sight, the brightness of their illumination only becomes apparent, through the rays being collected and reflected by some appropriate substance.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input, output_hidden_states=True)\n",
    "res = torch.cat(output[\"hidden_states\"][-3:-2], -1)[0].cpu()\n",
    "\n",
    "tk=tokenizer.tokenize(text)\n",
    "print(encoded_input[\"input_ids\"].shape, output.last_hidden_state.shape, len(tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁when', '▁these', '▁rays', '▁reach', '▁the', '▁observer', '▁direct', ',', '▁he', '▁sees', '▁the', '▁lamps', '▁or', '▁lumin', 'iferous', '▁bodies', '▁themselves', ',', '▁but', '▁when', '▁he', '▁is', '▁out', '▁of', '▁their', '▁direct', '▁sight', ',', '▁the', '▁brightness', '▁of', '▁their', '▁illumination', '▁only', '▁becomes', '▁apparent', ',', '▁through', '▁the', '▁rays', '▁being', '▁collected', '▁and', '▁reflected', '▁by', '▁some', '▁appropriate', '▁substance', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "converted=tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"][0])\n",
    "print(converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test for mapping from tokenizer output back to original text\n",
    "\n",
    "path=\"filelists/libritts_audio_sid_grapheme_all_filelist.txt.cleaned\"\n",
    "for l in open(path, 'r'):\n",
    "    text = l.split('|')[2]\n",
    "    encoded_input = tokenizer(text.strip(), return_tensors='pt')\n",
    "    converted=tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"][0])\n",
    "\n",
    "    tokens_nospecials = [c for c in converted if not(c in tokenizer.all_special_tokens)]\n",
    "    remove_whitespace = lambda c: (\" \" + c[1:]) if c[0] == \"▁\" else c\n",
    "    token_list = map(remove_whitespace, tokens_nospecials)\n",
    "    tokenized_str = (\"\".join(token_list)).strip()\n",
    "    if not (text.strip() == tokenized_str ):\n",
    "        print(f\"{text.strip()}\\n{tokenized_str}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 3, 3, 3],\n",
      "        [4, 4, 6, 6, 6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def repeat_columns(tensor, repeats):\n",
    "    # Ensure the tensor and repeats list are on the same device\n",
    "    tensor = tensor.to(repeats.device)\n",
    "\n",
    "    # Repeat the columns of the tensor\n",
    "    repeated_tensor = torch.repeat_interleave(tensor, repeats, dim=1)\n",
    "\n",
    "    return repeated_tensor\n",
    "\n",
    "# Original tensor\n",
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# List of repeats\n",
    "repeats = torch.tensor([2, 0, 3])\n",
    "\n",
    "# Call the function\n",
    "repeated_tensor = repeat_columns(tensor, repeats)\n",
    "\n",
    "# Print the repeated tensor\n",
    "print(repeated_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_bart_enc(text):\n",
    "    encoded_input = tokenizer(text.strip(), return_tensors='pt')\n",
    "    output = model(**encoded_input, output_hidden_states=True)\n",
    "    bart_out = torch.cat(output[\"hidden_states\"][-3:-2], -1)[0]\n",
    "\n",
    "    converted=tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"][0])\n",
    "\n",
    "    remove_whitespace = lambda c: (\" \" + c[1:]) if ((len(c)>0) and c[0] == \"▁\") else c\n",
    "\n",
    "    tokens_nospecials = [ (c if not(c in tokenizer.all_special_tokens) else \"\") for c in converted ]\n",
    "\n",
    "    token_list = list(map(remove_whitespace, tokens_nospecials))\n",
    "    if len(token_list[0])==0:\n",
    "        token_list[1] = token_list[1].strip()\n",
    "    repl_list = [len(token) for token in token_list]\n",
    "    \n",
    "    repeats = torch.tensor(repl_list, device=bart_out.device)\n",
    "    print(f\"{bart_out.shape=}\")\n",
    "    print(f\"{repeats.shape=}\")\n",
    "    repeated_tensor = torch.repeat_interleave(bart_out, repeats, dim=0)\n",
    "    # assert repeated_tensor.shape[0] == sum(repl_list)\n",
    "    print(f\"{repeated_tensor.shape=}, {sum(repl_list)=}\")\n",
    "    return (repeated_tensor, repl_list) #, token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bart_out.shape=torch.Size([19, 768])\n",
      "repeats.shape=torch.Size([19])\n",
      "repeated_tensor.shape=torch.Size([69, 768]), sum(repl_list)=69\n",
      "(tensor([[ 1.8150e-01, -5.4775e-01,  9.9657e-02,  ..., -6.1669e-01,\n",
      "          4.0106e-01,  1.0836e-01],\n",
      "        [ 1.8150e-01, -5.4775e-01,  9.9657e-02,  ..., -6.1669e-01,\n",
      "          4.0106e-01,  1.0836e-01],\n",
      "        [ 8.5165e-02, -9.8955e-01, -4.2447e-01,  ..., -9.3168e-01,\n",
      "          4.4037e-01,  2.4962e-01],\n",
      "        ...,\n",
      "        [-1.7115e+00,  2.4747e-02,  6.2423e-02,  ..., -1.5398e-01,\n",
      "          2.5467e-01,  8.1702e-01],\n",
      "        [-1.7115e+00,  2.4747e-02,  6.2423e-02,  ..., -1.5398e-01,\n",
      "          2.5467e-01,  8.1702e-01],\n",
      "        [-2.1153e-01, -1.8108e-01, -1.4590e-02,  ..., -2.3116e-02,\n",
      "         -7.8113e-04, -7.0608e-02]], grad_fn=<IndexSelectBackward0>), [0, 2, 6, 3, 5, 3, 2, 6, 7, 1, 2, 5, 3, 2, 6, 10, 5, 1, 0])\n",
      "bart_out.shape=torch.Size([25, 768])\n",
      "repeats.shape=torch.Size([25])\n",
      "repeated_tensor.shape=torch.Size([80, 768]), sum(repl_list)=80\n",
      "(tensor([[-0.0341, -0.5882,  0.3258,  ..., -0.7867,  0.5262,  0.1805],\n",
      "        [-0.0341, -0.5882,  0.3258,  ..., -0.7867,  0.5262,  0.1805],\n",
      "        [-0.0501, -0.5810, -0.3894,  ..., -0.7502,  1.0646,  0.3253],\n",
      "        ...,\n",
      "        [-0.5195,  0.0516, -0.3094,  ...,  0.0840,  0.7855,  0.6151],\n",
      "        [-0.5195,  0.0516, -0.3094,  ...,  0.0840,  0.7855,  0.6151],\n",
      "        [-0.2015, -0.1250, -0.0146,  ..., -0.1547,  0.0503, -0.1173]],\n",
      "       grad_fn=<IndexSelectBackward0>), [0, 2, 6, 3, 5, 3, 4, 6, 1, 4, 6, 3, 1, 3, 2, 6, 5, 1, 3, 5, 2, 5, 3, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "text=[\"at night we camp at a water pocket, a pool in a great limestone rock.\", \"at night we come to the cliff, and under it, in a great cave, we find a lakelet.\"]\n",
    "print(tokenize_bart_enc(text[0]))\n",
    "print(tokenize_bart_enc(text[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   313,   364,  1331,   262, 54213,   842,   261,  1661,  1365,\n",
       "           281,  1707,   263, 22644,   261,  4850,   314,   347, 12668,   576,\n",
       "           619,   293,   266,   426,   263,  9410,  3920,   267, 13753,   277,\n",
       "           266,  3759,   260,     2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] he only heard the heathen men, whose eyes are blue and bleak, singing about some cruel thing done by a great and smiling king in daylight on a deck.[SEP]'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "  w  h  e  n     t  h  e  s  e     r  a  y  s     r  e  a  c  h     t  h  e     o  b  s  e  r  v  e  r     d  i  r  e  c  t  ,     h  e     s  e  e  s     t  h  e     l  a  m  p  s     o  r     l  u  m  i  n  i  f  e  r  o  u  s     b  o  d  i  e  s     t  h  e  m  s  e  l  v  e  s  ,     b  u  t     w  h  e  n     h  e     i  s     o  u  t     o  f     t  h  e  i  r     d  i  r  e  c  t     s  i  g  h  t  ,     t  h  e     b  r  i  g  h  t  n  e  s  s     o  f     t  h  e  i  r     i  l  l  u  m  i  n  a  t  i  o  n     o  n  l  y     b  e  c  o  m  e  s     a  p  p  a  r  e  n  t  ,     t  h  r  o  u  g  h     t  h  e     r  a  y  s     b  e  i  n  g     c  o  l  l  e  c  t  e  d     a  n  d     r  e  f  l  e  c  t  e  d     b  y     s  o  m  e     a  p  p  r  o  p  r  i  a  t  e     s  u  b  s  t  a  n  c  e  ."
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "str=\"\"\n",
    "for c in converted:\n",
    "    if not(c in tokenizer.all_special_tokens):\n",
    "        if c[0] == \"▁\":\n",
    "            c = \" \"+c[1:]\n",
    "        str=str+c\n",
    "\n",
    "print(text==str.strip())\n",
    "print(\"\".join(difflib.ndiff(str.strip(), text)), end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when these rays reach the observer direct, he sees the lamps or luminiferous bodies themselves, but when he is out of their direct sight, the brightness of their illumination only becomes apparent, through the rays being collected and reflected by some appropriate substance.\n",
      " when these rays reach the observer direct, he sees the lamps or luminiferous bodies themselves, but when he is out of their direct sight, the brightness of their illumination only becomes apparent, through the rays being collected and reflected by some appropriate substance.\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 51, 768]) 49\n"
     ]
    }
   ],
   "source": [
    "print(output.last_hidden_state.shape, len(tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   335,   378, 12346,  1431,   262, 17450,  1670,   261,   313,\n",
       "          5649,   262, 10605,   289, 77697, 65372,  3362,  1147,   261,   304,\n",
       "           335,   313,   269,   321,   265,   308,  1670,  3941,   261,   262,\n",
       "         14436,   265,   308, 19956,   364,  2159,  5557,   261,   390,   262,\n",
       "         12346,   411,  3422,   263,  7034,   293,   347,  1825,  5182,   260,\n",
       "             2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtransformers\u001b[49m\u001b[38;5;241m.\u001b[39m__version__\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transformers' is not defined"
     ]
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁when', '▁these', '▁rays', '▁reach', '▁the', '▁observer', '▁direct', ',', '▁he', '▁sees', '▁the', '▁lamps', '▁or', '▁lumin', 'iferous', '▁bodies', '▁themselves', ',', '▁but', '▁when', '▁he', '▁is', '▁out', '▁of', '▁their', '▁direct', '▁sight', ',', '▁the', '▁brightness', '▁of', '▁their', '▁illumination', '▁only', '▁becomes', '▁apparent', ',', '▁through', '▁the', '▁rays', '▁being', '▁collected', '▁and', '▁reflected', '▁by', '▁some', '▁appropriate', '▁substance', '.']\n"
     ]
    }
   ],
   "source": [
    "tk=tokenizer.tokenize(text)\n",
    "print(tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "word_ids() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast` class).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[43mencoded_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(w)\n",
      "File \u001b[0;32m~/miniforge3/envs/TTS/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:380\u001b[0m, in \u001b[0;36mBatchEncoding.word_ids\u001b[0;34m(self, batch_index)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03mReturn a list mapping the tokens to their actual word in the initial sentence for a fast tokenizer.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03m    (several tokens will be mapped to the same word index if they are parts of that word).\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings:\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_ids() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m     )\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings[batch_index]\u001b[38;5;241m.\u001b[39mword_ids\n",
      "\u001b[0;31mValueError\u001b[0m: word_ids() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast` class)."
     ]
    }
   ],
   "source": [
    "for w in encoded_input.word_ids():\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1=torch.cat(output[\"hidden_states\"][-3:-2], -1)[0]\n",
    "output1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'torch.Size' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'torch.Size' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Invalid key. Only three types of key are available: (1) string, (2) integers for backend Encoding, and (3) slices for data subsetting.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mencoded_input\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/TTS/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:259\u001b[0m, in \u001b[0;36mBatchEncoding.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[key][item] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mkeys()}\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid key. Only three types of key are available: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(1) string, (2) integers for backend Encoding, and (3) slices for data subsetting.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m     )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Invalid key. Only three types of key are available: (1) string, (2) integers for backend Encoding, and (3) slices for data subsetting.'"
     ]
    }
   ],
   "source": [
    "encoded_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.7815e-02, -6.4330e-02, -9.3293e-02,  ...,  1.8593e-02,\n",
       "           2.0229e-02,  4.8493e-02],\n",
       "         [-1.0126e+00, -5.7708e-01,  2.5398e-01,  ...,  8.8124e-01,\n",
       "           4.5454e-01, -2.3135e-01],\n",
       "         [-2.3604e-01,  5.8270e-01, -1.4709e-01,  ...,  9.6723e-01,\n",
       "           4.0348e-01, -2.1966e-01],\n",
       "         ...,\n",
       "         [ 4.9370e-01, -4.5198e-02,  2.3887e-01,  ...,  8.6032e-01,\n",
       "           5.7410e-01, -6.6281e-04],\n",
       "         [-9.7019e-02, -2.0085e-01, -1.0030e-01,  ...,  2.6730e-01,\n",
       "          -5.6538e-03,  2.1169e-03],\n",
       "         [ 6.7819e-02, -4.0412e-02, -6.2213e-02,  ...,  1.7648e-02,\n",
       "          -2.8052e-03,  1.1247e-02]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=torch.cat(output[\"hidden_states\"][-3:-2], -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output['hidden_states'][-3:-2])[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pylab as pl\n",
    "pl.imshow(output.last_hidden_state.detach().numpy()[0,:,:].T)\n",
    "encoded_input.input_ids.shape\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0082,  0.0093, -0.0317,  ..., -0.0706, -0.0346, -0.0810],\n",
       "         [-1.0421, -0.2735,  0.1558,  ...,  0.4248,  0.5864,  0.4635],\n",
       "         [ 0.4201, -0.1396, -0.2986,  ...,  0.2817, -0.2221, -0.1911],\n",
       "         ...,\n",
       "         [ 0.8387, -0.2888,  0.7874,  ...,  0.2854,  0.4556,  0.4273],\n",
       "         [-0.2095,  0.1081,  0.3647,  ...,  0.3607,  0.0144, -0.0899],\n",
       "         [ 0.0027,  0.0066, -0.0280,  ..., -0.0765, -0.0286, -0.0639]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[\"Hellow World\", \"Good nights\", \"What is going on?\"]\n",
    "encoded_input = tokenizer(texts, return_tensors='pt', padding=True)\n",
    "\n",
    "output = model(**encoded_input, output_hidden_states=True)\n",
    "bart_out = torch.cat(output[\"hidden_states\"][-3:-2], -1)[0]\n",
    "converted=tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 17, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_bert_enc(texts):\n",
    "    encoded_input = tokenizer(texts, return_tensors='pt', padding=True)\n",
    "    output = model(**encoded_input, output_hidden_states=True)\n",
    "    bart_out = torch.cat(output[\"hidden_states\"][-3:-2], -1)[0]\n",
    "\n",
    "    remove_whitespace = lambda c: (\" \" + c[1:]) if ((len(c)>0) and c[0] == \"▁\") else c\n",
    "    all_tensors = []\n",
    "    for encoded in encoded_input[\"input_ids\"]:\n",
    "        converted=tokenizer.convert_ids_to_tokens(encoded)\n",
    "        tokens_nospecials = [ (c if not(c in tokenizer.all_special_tokens) else \"\") for c in converted ]\n",
    "\n",
    "        token_list = list(map(remove_whitespace, tokens_nospecials))\n",
    "        if len(token_list[0])==0:\n",
    "            token_list[1] = token_list[1].strip()\n",
    "        repl_list = [len(token) for token in token_list]\n",
    "    \n",
    "        repeats = torch.tensor(repl_list, device=bart_out.device)\n",
    "        # print(f\"{bart_out.shape=}\")\n",
    "        # print(f\"{repeats.shape=}\")\n",
    "        repeated_tensor = torch.repeat_interleave(bart_out, repeats, dim=0)\n",
    "        # assert repeated_tensor.shape[0] == sum(repl_list), f\"{text}: {bart_out.shape=}, {repeated_tensor.shape=}, {sum(repl_list)=}\"\n",
    "        # print(f\"{repeated_tensor.shape=}, {sum(repl_list)=}\")\n",
    "        all_tensors.append(repeated_tensor)\n",
    "\n",
    "    max_rows = max(tensor.size(0) for tensor in all_tensors)\n",
    "    padded_tensors = [torch.nn.functional.pad(tensor, (0, 0, 0, max_rows - tensor.size(0))) for tensor in all_tensors]\n",
    "    stacked_tensors = torch.stack(padded_tensors, dim=0)\n",
    "\n",
    "    return stacked_tensors\n",
    "\n",
    "stacked_tensors=tokenize_bert_enc(texts)\n",
    "stacked_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 11, 17]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 768])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 47, 54, 54, 57]\n",
      "Hello\n"
     ]
    }
   ],
   "source": [
    "from text import cleaned_text_to_sequence, sequence_to_text\n",
    "\n",
    "txt = \"Hello\"\n",
    "\n",
    "sequence = cleaned_text_to_sequence(txt)\n",
    "print(sequence)\n",
    "print(sequence_to_text(sequence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sherpa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
